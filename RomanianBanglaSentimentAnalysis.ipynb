{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMLIy3PhOaBgwhlgtM5soHq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ashibullah/Romanian-Bangla-Sentiment-Analysis-NLP/blob/main/RomanianBanglaSentimentAnalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "-h08gX8Df4Cg"
      },
      "outputs": [],
      "source": [
        "!pip install datasets --upgrade"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "id": "tz3SF_EKWkF_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')  # For tokenizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkpH-xM9WnC-",
        "outputId": "31256daa-cfd5-4f1e-a30f-576608ceb090"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "is9rz2vygTFi"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = load_dataset(\"aplycaebous/BnSentMix\" , split = \"train\")"
      ],
      "metadata": {
        "id": "x1eXqhC1iL0C",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zRBhvPHnvEUQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    return text"
      ],
      "metadata": {
        "id": "hILT6_kjsRpI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def correct_text(text):\n",
        "    return str(TextBlob(text).correct())"
      ],
      "metadata": {
        "id": "hi1EvFEZvORq"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df[20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jDl8MTN4ifFO",
        "outputId": "7ed3f345-17f2-4121-9a24-b2678a4225eb"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'Sentence': 'Vloi bt cash out korte 15 takar jaigai 18.50 paisa nei', 'Label': 3}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataFrame\n",
        "df = pd.DataFrame(df)\n",
        "# Convert 'Sentence' column to lowercase\n",
        "df['Sentence'] = df['Sentence'].str.lower()\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "aAajerISpyT4",
        "outputId": "3251fbcb-f984-41c0-afe4-a4275bf6f83c"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Sentence  Label  \\\n",
            "0      youtube ar volg gula boring hoia jaitase din d...      3   \n",
            "1      your video making camera work is really good i...      3   \n",
            "2      you made me nostalgic college life a ei dokan ...      3   \n",
            "3      workshop ta engaging but resources ta insuffic...      3   \n",
            "4                      win hoy nay but anek valo khelecu      3   \n",
            "...                                                  ...    ...   \n",
            "20010  1 march use kortasi pocof5 kono problem nai ga...      0   \n",
            "20011                       1 day beshi stay kora jae na      0   \n",
            "20012            1 boro na 2 boro tushar vai er mon boro      0   \n",
            "20013               1 boro na 2 boro sam vai er mon boro      0   \n",
            "20014  058 pcbbd er depression boys mone hoillo minar...      0   \n",
            "\n",
            "                                                  Tokens  \n",
            "0      [youtube, ar, volg, gula, boring, hoia, jaitas...  \n",
            "1      [your, video, making, camera, work, is, really...  \n",
            "2      [you, made, me, nostalgic, college, life, a, e...  \n",
            "3      [workshop, ta, engaging, but, resources, ta, i...  \n",
            "4              [win, hoy, nay, but, anek, valo, khelecu]  \n",
            "...                                                  ...  \n",
            "20010  [1, march, use, kortasi, pocof5, kono, problem...  \n",
            "20011               [1, day, beshi, stay, kora, jae, na]  \n",
            "20012  [1, boro, na, 2, boro, tushar, vai, er, mon, b...  \n",
            "20013    [1, boro, na, 2, boro, sam, vai, er, mon, boro]  \n",
            "20014  [058, pcbbd, er, depression, boys, mone, hoill...  \n",
            "\n",
            "[20015 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# remove urls\n",
        "df['Sentence'] = df['Sentence'].apply(clean_text)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "ZQZ_1uMpsNdc",
        "outputId": "9c4f058e-5a47-4a2e-aa99-3e353ca930ec"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Sentence  Label  \\\n",
            "0      youtube ar volg gula boring hoia jaitase din d...      3   \n",
            "1      your video making camera work is really good i...      3   \n",
            "2      you made me nostalgic college life a ei dokan ...      3   \n",
            "3      workshop ta engaging but resources ta insuffic...      3   \n",
            "4                      win hoy nay but anek valo khelecu      3   \n",
            "...                                                  ...    ...   \n",
            "20010  1 march use kortasi pocof5 kono problem nai ga...      0   \n",
            "20011                       1 day beshi stay kora jae na      0   \n",
            "20012            1 boro na 2 boro tushar vai er mon boro      0   \n",
            "20013               1 boro na 2 boro sam vai er mon boro      0   \n",
            "20014  058 pcbbd er depression boys mone hoillo minar...      0   \n",
            "\n",
            "                                                  Tokens  \n",
            "0      [youtube, ar, volg, gula, boring, hoia, jaitas...  \n",
            "1      [your, video, making, camera, work, is, really...  \n",
            "2      [you, made, me, nostalgic, college, life, a, e...  \n",
            "3      [workshop, ta, engaging, but, resources, ta, i...  \n",
            "4              [win, hoy, nay, but, anek, valo, khelecu]  \n",
            "...                                                  ...  \n",
            "20010  [1, march, use, kortasi, pocof5, kono, problem...  \n",
            "20011               [1, day, beshi, stay, kora, jae, na]  \n",
            "20012  [1, boro, na, 2, boro, tushar, vai, er, mon, b...  \n",
            "20013    [1, boro, na, 2, boro, sam, vai, er, mon, boro]  \n",
            "20014  [058, pcbbd, er, depression, boys, mone, hoill...  \n",
            "\n",
            "[20015 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# df = df['Sentence'].apply(correct_text)\n",
        "# avoiding this"
      ],
      "metadata": {
        "id": "Kjw1OlD-ua1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize using NLTK\n",
        "df['Tokens'] = df['Sentence'].apply(word_tokenize)\n",
        "\n",
        "print(df[['Label']])"
      ],
      "metadata": {
        "id": "ShtMtGVXXcVE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07169054-e8d8-4590-a0bf-a6abf0f5c970"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Label\n",
            "0          3\n",
            "1          3\n",
            "2          3\n",
            "3          3\n",
            "4          3\n",
            "...      ...\n",
            "20010      0\n",
            "20011      0\n",
            "20012      0\n",
            "20013      0\n",
            "20014      0\n",
            "\n",
            "[20015 rows x 1 columns]\n"
          ]
        }
      ]
    }
  ]
}